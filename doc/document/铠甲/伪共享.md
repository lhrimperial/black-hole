#### 伪共享

##### 前言

在现代多核CPU架构下的并发编程中，如果将互斥锁的竞争比作“性能杀手”的话，那么伪共享就相当于“性能刺客”。“杀手”与“刺客”本质的区别在于杀手是可见的，对于可见的杀手可以有多种方式手段去应付，但是“刺客”却不同，“刺客”永远隐藏在暗处，伺机给人致命一击，防不胜防。具体到我们的并发编程中，遇到锁竞争影响并发性能情况时，我们可以采取多种措施（如缩短临界区，原子操作等等）去提高程序性能，但是伪共享却是我们从所写代码中看不出任何蛛丝马迹的，发现不了问题也就无法解决问题，从而导致伪共享在“暗处”严重拖累程序的并发性能，但我们却束手无策。



##### CPU Cache概述

随着CPU的频率不断提升，而内存的访问速度却没有质的突破，为了弥补访问内存的速度慢，充分发挥CPU的计算资源，提高CPU整体吞吐量，在CPU与内存之间引入了一级Cache。随着热点数据体积越来越大，一级Cache L1已经不满足发展的要求，引入了二级Cache L2，三级Cache L3。CPU Cache存储器层次结构：

![CPU_Cache-存储层级结构](伪共享.assets/CPU_Cache-存储层级结构.svg)

现在的计算机早已进入多核时代，软件也越来越多的支持多核运行。一个处理器对应一个物理插槽，多个处理器之间通过QPI总线相连。一个处理器包含多个核，一个处理器间的多核共享L3 Cache。一个核包含寄存器、L1 Cache、L2 Cache，下图是Intel Sandy Bridge CPU架构，一个典型的NUMA多处理器结构：

![CPU_Cache-多核CPU结构](伪共享.assets/CPU_Cache-多核CPU结构.svg)

作为程序员，需要理解计算机存储器层次结构，它对应用程序的性能有巨大的影响。

1. 各种寄存器，用来存储本地变量和函数参数，访问一次需要1cycle，耗时小于1ns；
2. L1 Cache，一级缓存，本地 core 的缓存，分成 32K 的数据缓存 L1d 和 32k 指令缓存 L1i，访问 L1 需要3cycles，耗时大约 1ns；
3. L2 Cache，二级缓存，本地 core 的缓存，被设计为 L1 缓存与共享的 L3 缓存之间的缓冲，大小为 256K，访问 L2 需要 12cycles，耗时大约 3ns；
4. L3 Cache，三级缓存，在同插槽的所有 core 共享 L3 缓存，分为多个 2M 的段，访问 L3 需要 38cycles，耗时大约 12ns；

大致可以得出结论，缓存层级越接近于 CPU core，容量越小，速度越快，同时，没有披露的一点是其造价也更贵。所以为了支撑更多的热点数据，同时追求最高的性价比，多级缓存架构应运而生。

##### Cache Line

缓存行 (Cache Line) 便是 CPU Cache 中的最小单位，CPU Cache 由若干缓存行组成，一个缓存行的大小通常是 64 字节（这取决于 CPU）。每个Cache Line又额外包含一个有效位(valid bit)、t个标记位(tag bit)，其中valid bit用来表示该缓存行是否有效；tag bit用来协助寻址，唯一标识存储在CacheLine中的块；而Cache Line里的64个字节其实是对应内存地址中的数据拷贝。

![CPU_Cache-Cache Line](伪共享.assets/CPU_Cache-Cache Line.svg)

##### 伪共享

伪共享指的是多个线程同时读写同一个缓存行的不同变量时导致的 CPU 缓存失效。尽管这些变量之间没有任何关系，但由于在主内存中邻近，加载并存储于同一个缓存行之中，它们的相互覆盖会导致频繁的缓存未命中，引发性能下降。伪共享问题难以被定位，如果系统设计者不理解 CPU 缓存架构，甚至永远无法发现 — 原来我的程序还可以更快。

![CPU_Cache-伪共享](伪共享.assets/CPU_Cache-伪共享.svg)

如果多个线程的变量共享了同一个 CacheLine，任意一方的修改操作都会使得整个 CacheLine 失效（因为 CacheLine 是 CPU 缓存的最小单位），也就意味着，频繁的多线程操作，CPU 缓存将会彻底失效，降级为 CPU core 和主内存的直接交互。

#####如何避免伪共享
避免伪共享主要有以下两种方式：

- 缓存行填充（Padding）：为了避免伪共享就需要将可能造成伪共享的多个变量处于不同的缓存行中，可以采用在变量后面填充字节的方式达到该目的。
- 使用某些语言或编译器中强制变量对齐，将变量都对齐到缓存行大小，避免伪共享发生。

###### 伪共享-字节填充

1. Java6 中实现字节填充

   ```java
   public class PaddingObject{
       public volatile long value = 0L;    // 实际数据
       public long p1, p2, p3, p4, p5, p6; // 填充
   }
   ```

   PaddingObject 类中需要保存一个 long 类型的 value 值，如果多线程操作同一个 CacheLine 中的 PaddingObject 对象，便无法完全发挥出 CPU Cache 的优势（想象一下你定义了一个 PaddingObject[] 数组，数组元素在内存中连续，却由于伪共享导致无法使用 CPU Cache ）。在 Java 中，**对象头还占据了 8 个字节**，所以一个 PaddingObject 对象可以恰好占据一个 Cache Line。

2. Java7 中实现字节填充

   在 Java7 之后，一个 JVM 的优化给字节填充造成了一些影响，上面的代码片段 `public long p1, p2, p3, p4, p5, p6;` 会被认为是无效代码被优化掉，有回归到了伪共享的窘境之中。

   为了避免 JVM 的自动优化，需要使用继承的方式来填充。

   ```java
   abstract class AbstractPaddingObject{
       protected long p1, p2, p3, p4, p5, p6;// 填充
   }
   
   public class PaddingObject extends AbstractPaddingObject{
       public volatile long value = 0L;    // 实际数据
   }
   ```

3. Java8 中实现字节填充

   ```java
   @Retention(RetentionPolicy.RUNTIME)
   @Target({ElementType.FIELD, ElementType.TYPE})
   public @interface Contended {
       String value() default "";
   }
   ```

   **注意需要同时开启 JVM 参数：-XX:-RestrictContended=false**

   Java8 中提供了字节填充的官方实现，这无疑使得 CPU Cache 更加可控了，无需担心 jdk 的无效字段优化，无需担心 Cache Line 在不同 CPU 下的大小究竟是不是 64 字节。使用 @Contended 注解可以完美的避免伪共享问题。

   > @Contended 注解会增加目标实例大小，要谨慎使用。默认情况下，除了 JDK 内部的类，JVM 会忽略该注解。要应用代码支持的话，要设置 -XX:-RestrictContended=false，它默认为 true（意味仅限 JDK 内部的类使用）。当然，也有个 –XX: EnableContented 的配置参数，来控制开启和关闭该注解的功能，默认是 true，如果改为 false，可以减少 Thread 和 ConcurrentHashMap 类的大小。

#####附录

MESI协议是一个基于失效的缓存一致性协议，是支持回写（write-back）缓存的最常用协议。也称作伊利诺伊协议 (Illinois protocol，因为是在伊利诺伊大学厄巴纳-香槟分校被发明的)。与写通过（write through）缓存相比，回写缓冲能节约大量带宽。总是有“脏”（dirty）状态表示缓存中的数据与主存中不同。MESI协议要求在缓存不命中（miss）且数据块在另一个缓存时，允许缓存到缓存的数据复制。与MSI协议相比，MESI协议减少了主存的事务数量。这极大改善了性能。

缓存行有4种不同的状态:

- 已修改Modified (M)
  缓存行是脏的（dirty），与主存的值不同。如果别的CPU内核要读主存这块数据，该缓存行必须回写到主存，状态变为共享(S).
- 独占Exclusive (E)
  缓存行只在当前缓存中，但是干净的（clean）--缓存数据同于主存数据。当别的缓存读取它时，状态变为共享；当前写数据时，变为已修改状态。
- 共享Shared (S)
  缓存行也存在于其它缓存中且是干净的。缓存行可以在任意时刻抛弃。
- 无效Invalid (I)
  缓存行是无效的



##### 参考资料

https://blog.csdn.net/qq_27680317/article/details/78486220

https://www.cnkirito.moe/cache-line/